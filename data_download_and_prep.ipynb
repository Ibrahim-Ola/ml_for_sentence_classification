{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# see https://spacy.io/usage for proper spacy installation\n",
    "import spacy\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # load text processing pipeline from spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data we will use can be found [here](https://cogcomp.seas.upenn.edu/Data/QA/QC/). The dataset comprises three variables: `question`, `category`, and `sub-category`. However, our analysis will only use two variables: `question` and `category`. Essentially, we will build a model that uses the content of the question to predict its category. For example, (e.g. Who was Abraham Lincon?) and the output or label would be Human. There are six disnnct categories in the dataset:\n",
    "\n",
    "1. ENTY (Entity): Questions that seek specific entities as answers, like objects, organisms, or concepts.\n",
    "2. HUM (Human): Questions about humans, individually or as a group.\n",
    "3. DESC (Description): Questions asking for description, explanations, or reasons.\n",
    "4. Num (Numeric): Questions expecting numerical answer.\n",
    "4. LOC (Locanon): Questions that are geographically oriented.\n",
    "6. ABBR (Abbreviation): Questions seeking the extended form or explanation of abbreviations.\n",
    "\n",
    "For this work, we will use the Training Set 5 dataset for training the model and the single test set available for testing the model.\n",
    "\n",
    "We will now create the data download utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = './data'\n",
    "os.makedirs(dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(dir_name: str, filename: str, url: str, expected_bytes: Optional[int]=None) -> str:\n",
    "    \"\"\"\n",
    "    Download a file if not present, and make sure it's the right size if the expected size is provided.\n",
    "    \n",
    "    Args:\n",
    "        dir_name (str): The directory where the data will be stored.\n",
    "        filename (str): The filename under which the data will be stored.\n",
    "        url (str): The URL from which to download the data.\n",
    "        expected_bytes (Optional[int]): The expected size of the data in bytes. \n",
    "                                        If provided, the function will check if the downloaded file size matches this.\n",
    "                                        If not, an exception will be raised.\n",
    "                                        If not provided, no size check will be performed.\n",
    "                                        \n",
    "    Returns:\n",
    "        str: The file path where the data is stored.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Define the filepath\n",
    "    filepath = os.path.join(dir_name, filename)\n",
    "\n",
    "    # Download the file if it doesn't already exist\n",
    "    if not os.path.exists(filepath):\n",
    "        response = requests.get(url)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            # Write the content of the response to filepath\n",
    "            f.write(response.content)\n",
    "\n",
    "    # If an expected size is provided, verify the size of the downloaded file\n",
    "    if expected_bytes is not None:   \n",
    "        statinfo = os.stat(filepath)\n",
    "        if statinfo.st_size == expected_bytes:\n",
    "            print(f'Found and verified {filepath}')\n",
    "        else:\n",
    "            print(f'File size {statinfo.st_size} does not match expected size {expected_bytes}')\n",
    "            raise Exception(\n",
    "              f'Failed to verify {filepath}. Can you get to it with a browser?')\n",
    "    \n",
    "    # Return the filepath for use elsewhere\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data/train_5500.label\n",
      "Found and verified data/TREC_10.label\n"
     ]
    }
   ],
   "source": [
    "# Download the data.\n",
    "url = 'http://cogcomp.org/Data/QA/QC/'\n",
    "dir_name = 'data'\n",
    "train_filename = download_data(dir_name, 'train_5500.label', url+'train_5500.label', 335858)\n",
    "test_filename = download_data(dir_name, 'TREC_10.label', url+'TREC_10.label', 23354)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Read data from a file with given filename.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing three lists of strings representing questions, categories, and sub-categories, respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    questions, categories, sub_categories = [], [], []\n",
    "    \n",
    "    try:\n",
    "        with open(filename,'r',encoding='latin-1') as f:        \n",
    "            for row in f:\n",
    "                # Check if the row can be split into category and sub_category+question\n",
    "                if ':' in row:\n",
    "                    cat, sub_cat_and_question = row.split(\":\", 1)\n",
    "                    tokens = sub_cat_and_question.split(' ')\n",
    "                    sub_cat, question = tokens[0], ' '.join(tokens[1:])\n",
    "                    \n",
    "                    questions.append(question.strip())\n",
    "                    categories.append(cat)\n",
    "                    sub_categories.append(sub_cat)\n",
    "                else:\n",
    "                    print(f\"Skipping invalid row: {row}\")\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {filename} was not found.\")\n",
    "        \n",
    "    return questions, categories, sub_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's inspect Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "===========\n",
      "Data has 5452 questions and 5452 labels\n",
      "Sample Observations:\n",
      "\tQuestion - How did serfdom develop in and then leave Russia ? / Category - DESC / Sub-category - manner\n",
      "\tQuestion - What films featured the character Popeye Doyle ? / Category - ENTY / Sub-category - cremat\n",
      "\tQuestion - How can I find a list of celebrities ' real names ? / Category - DESC / Sub-category - manner\n",
      "\tQuestion - What fowl grabs the spotlight after the Chinese Year of the Monkey ? / Category - ENTY / Sub-category - animal\n",
      "\tQuestion - What is the full form of .com ? / Category - ABBR / Sub-category - exp\n",
      "\n",
      "\n",
      "Test Data:\n",
      "==========\n",
      "Data has 500 questions and 500 labels\n",
      "Sample Observations:\n",
      "\tQuestion - How far is it from Denver to Aspen ? / Category - NUM / Sub-category - dist\n",
      "\tQuestion - What county is Modesto , California in ? / Category - LOC / Sub-category - city\n",
      "\tQuestion - Who was Galileo ? / Category - HUM / Sub-category - desc\n",
      "\tQuestion - What is an atom ? / Category - DESC / Sub-category - def\n",
      "\tQuestion - When did Hawaii become a state ? / Category - NUM / Sub-category - date\n"
     ]
    }
   ],
   "source": [
    "def print_samples(questions: List[str], categories: List[str], sub_categories: List[str], n_samples: int):\n",
    "    \"\"\"\n",
    "    Print samples from the questions, categories, and sub_categories lists.\n",
    "\n",
    "    Args:\n",
    "        questions: A list of questions.\n",
    "        categories: A list of categories corresponding to the questions.\n",
    "        sub_categories: A list of sub_categories corresponding to the questions.\n",
    "        n_samples: The number of samples to print.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Data has {len(questions)} questions and {len(categories)} labels\")\n",
    "    print(\"Sample Observations:\")\n",
    "    for question, cat, sub_cat in zip(questions[:n_samples], categories[:n_samples], sub_categories[:n_samples]):\n",
    "        print(f\"\\tQuestion - {question} / Category - {cat} / Sub-category - {sub_cat}\")\n",
    "\n",
    "train_questions, train_categories, train_sub_categories = read_data(train_filename)\n",
    "test_questions, test_categories, test_sub_categories = read_data(test_filename)\n",
    "\n",
    "n_samples = 5\n",
    "print(\"Train Data:\")\n",
    "print(\"===========\")\n",
    "print_samples(train_questions, train_categories, train_sub_categories, n_samples)\n",
    "print('\\n')\n",
    "print(\"Test Data:\")\n",
    "print(\"==========\")\n",
    "print_samples(test_questions, test_categories, test_sub_categories, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Trand and Test Data to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5452 entries, 0 to 5451\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   question      5452 non-null   object\n",
      " 1   category      5452 non-null   object\n",
      " 2   sub_category  5452 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 127.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>cremat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>ABBR</td>\n",
       "      <td>exp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question category sub_category\n",
       "0  How did serfdom develop in and then leave Russ...     DESC       manner\n",
       "1   What films featured the character Popeye Doyle ?     ENTY       cremat\n",
       "2  How can I find a list of celebrities ' real na...     DESC       manner\n",
       "3  What fowl grabs the spotlight after the Chines...     ENTY       animal\n",
       "4                    What is the full form of .com ?     ABBR          exp"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train and test data\n",
    "\n",
    "train_data = pd.DataFrame({\n",
    "    'question': train_questions, \n",
    "    'category': train_categories, \n",
    "    'sub_category': train_sub_categories\n",
    "})\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'question': test_questions, \n",
    "    'category': test_categories,\n",
    "    'sub_category': test_sub_categories\n",
    "})\n",
    "\n",
    "train_data.info()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Exploration\n",
    "\n",
    "\n",
    "Let's create a function to clean our data before exploration. See the docstring for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text: str) -> List[str]:\n",
    "    \"\"\"Performs lemmatization on a text.\"\"\"\n",
    "    return [word.lemma_ for word in nlp(text)]\n",
    "\n",
    "## get stop words\n",
    "stop_words = ENGLISH_STOP_WORDS.union(STOP_WORDS)\n",
    "stop_words = {words.replace(\"'\", \"\") for words in stop_words}\n",
    "\n",
    "def clean_questions(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans up a question.\n",
    "\n",
    "    - Replaces all new lines (\\n) with white space.\n",
    "    - Converts all words to lower case.\n",
    "    - Removes all characters except a-z, A-Z, hyphens and white space.\n",
    "    - Removes hyphens that are not part of a word.\n",
    "    - Removes extra white spaces.\n",
    "    - Removes stop words.\n",
    "    - Performs lemmatization.\n",
    "    \"\"\"\n",
    "    question = question.replace('\\n', ' ').lower()  # replace newlines and convert to lowercase\n",
    "    question = re.sub('[^-a-z ]', '', question)  # keep only letters, hyphens, and white space\n",
    "    question = re.sub('(?<=[^a-z])-(?=[^a-z])', '', question)  # remove hyphens not part of a word\n",
    "    question = re.sub(' +', ' ', question)  # replace multiple spaces with a single space\n",
    "    question = ' '.join([word for word in question.split() if word not in stop_words])  # remove stop words\n",
    "    question = lemmatizer(question)  # lemmatization\n",
    "    return ' '.join(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken (mins): 0.38 \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_data.loc[:, 'cleaned'] = train_data.loc[:, 'question'].apply(clean_questions) \n",
    "print('Total time taken (mins): {:.2f} '.format(float(time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
