{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Union\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, PredefinedSplit\n",
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "    accuracy_score, \n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    ConfusionMatrixDisplay,\n",
    "    balanced_accuracy_score, \n",
    "    average_precision_score\n",
    ") \n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # set to 1 to avoid tensorflow warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5452 entries, 0 to 5451\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   question      5452 non-null   object\n",
      " 1   category      5452 non-null   object\n",
      " 2   sub_category  5452 non-null   object\n",
      " 3   cleaned       5452 non-null   object\n",
      " 4   wordlengths   5452 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 213.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>wordlengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "      <td>serfdom develop leave russia</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>cremat</td>\n",
       "      <td>film feature character popeye doyle</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "      <td>list celebrity real name</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>animal</td>\n",
       "      <td>fowl grab spotlight chinese year monkey</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>ABBR</td>\n",
       "      <td>exp</td>\n",
       "      <td>form com</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question category sub_category  \\\n",
       "0  How did serfdom develop in and then leave Russ...     DESC       manner   \n",
       "1   What films featured the character Popeye Doyle ?     ENTY       cremat   \n",
       "2  How can I find a list of celebrities ' real na...     DESC       manner   \n",
       "3  What fowl grabs the spotlight after the Chines...     ENTY       animal   \n",
       "4                    What is the full form of .com ?     ABBR          exp   \n",
       "\n",
       "                                   cleaned  wordlengths  \n",
       "0             serfdom develop leave russia           10  \n",
       "1      film feature character popeye doyle            8  \n",
       "2                 list celebrity real name           12  \n",
       "3  fowl grab spotlight chinese year monkey           13  \n",
       "4                                 form com            8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_pickle('./data/train_data.pkl')\n",
    "test_data = pd.read_pickle('./data/test_data.pkl')\n",
    "\n",
    "train_data.info()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embedding Transformer\n",
    "\n",
    "We will now create a class for generating embeddings from text data using pre-trained language models. Specifically, we will use two language models:\n",
    "\n",
    "1. Bidirectional Encoder Representations from Transformers (BERT) Model by Google (read more [here](https://huggingface.co/google-bert/bert-base-uncased)).\n",
    "2. google/nnlm: Token based text embeddings trained on various Google News datasets (read more [here](https://www.kaggle.com/models/google/nnlm/frameworks/tensorFlow2/variations/en-dim50/versions/1?tfhub-redirect=true))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer for generating embeddings from text data using pre-trained language models.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained language model to use for generating embeddings.\n",
    "                          If \"nnlm-en-dim50\" is in the model_name, the corresponding TensorFlow Hub model will be used.\n",
    "                          Otherwise, a Hugging Face Transformers model will be used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"bert-base-uncased\"):\n",
    "        self.model_name = model_name\n",
    "        self._validate_model_name()\n",
    "\n",
    "        if \"nnlm-en-dim50\" in self.model_name:\n",
    "            self.embed = hub.load(\"https://www.kaggle.com/models/google/nnlm/frameworks/TensorFlow2/variations/en-dim128/versions/1\")\n",
    "        else:\n",
    "            self._setup_transformers_model()\n",
    "\n",
    "    def _validate_model_name(self):\n",
    "        \"\"\"Check if the specified model_name is valid.\"\"\"\n",
    "        if not isinstance(self.model_name, str):\n",
    "            raise ValueError(\"model_name must be a string\")\n",
    "\n",
    "    def _setup_transformers_model(self):\n",
    "        \"\"\"Load the Hugging Face Transformers model and set up the appropriate device.\"\"\"\n",
    "        self.device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name).to(self.device)\n",
    "\n",
    "    def fit(self, X: Union[pd.Series, pd.DataFrame], y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer (required for scikit-learn compatibility).\n",
    "\n",
    "        Args:\n",
    "            X (Union[pd.Series, pd.DataFrame]): The input data (not used for this transformer).\n",
    "            y (optional): The target data (not used for this transformer).\n",
    "\n",
    "        Returns:\n",
    "            self: The EmbeddingTransformer instance.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: Union[pd.Series, pd.DataFrame]):\n",
    "        \"\"\"\n",
    "        Generate embeddings from input text data using the specified pre-trained language model.\n",
    "\n",
    "        Args:\n",
    "            X (Union[pd.Series, pd.DataFrame]): The input text data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A Pandas DataFrame containing the generated embeddings.\n",
    "        \"\"\"\n",
    "        self._validate_input_data(X)\n",
    "\n",
    "        if \"nnlm-en-dim50\" in self.model_name:\n",
    "            return self._transform_with_nnlm(X)\n",
    "        else:\n",
    "            return self._transform_with_transformers(X)\n",
    "\n",
    "    def _validate_input_data(self, X: Union[pd.Series, pd.DataFrame]):\n",
    "        \"\"\"Check if the input data is valid.\"\"\"\n",
    "        if not isinstance(X, (pd.Series, pd.DataFrame)):\n",
    "            raise ValueError(\"Input data must be a Pandas Series or DataFrame\")\n",
    "\n",
    "    def _transform_with_nnlm(self, X: Union[pd.Series, pd.DataFrame]):\n",
    "        \"\"\"Generate embeddings using the TensorFlow Hub NNLM model.\"\"\"\n",
    "        text_data = X.tolist()\n",
    "        embeddings = self.embed(text_data).numpy()\n",
    "        return pd.DataFrame(embeddings)\n",
    "\n",
    "    def _transform_with_transformers(self, X: Union[pd.Series, pd.DataFrame]):\n",
    "        \"\"\"Generate embeddings using a Hugging Face Transformers model.\"\"\"\n",
    "        batch_text = X.tolist()\n",
    "        inputs = self.tokenizer.batch_encode_plus(\n",
    "            batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=22\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        return pd.DataFrame(embeddings)\n",
    "\n",
    "## create DenseTransformer class to convert sparse matrix to dense matrix\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 12:34:59.800380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1073 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-03-10 12:34:59.801035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21282 MB memory:  -> device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Initialize the vectorizers\n",
    "default_vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(stop_words='english', ngram_range=(1, 2), max_features=1024),\n",
    "    'TfidfVectorizer': TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=1024),\n",
    "    'bert-base-uncased': EmbeddingTransformer('bert-base-uncased'),\n",
    "    'nnlm-en-dim128': EmbeddingTransformer('nnlm-en-dim50')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(\n",
    "    trad_model,\n",
    "    parameters,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    vectorizers=None,\n",
    "    cv: Optional[PredefinedSplit] = None,\n",
    "):\n",
    "    results = []\n",
    "    best_preds_train = {}\n",
    "    best_preds_test = {}\n",
    "    current_best = {\n",
    "        'accuracy': 0,\n",
    "        'model': None,\n",
    "        'params': None\n",
    "    }\n",
    "\n",
    "    if vectorizers is None:\n",
    "        vectorizers = default_vectorizers\n",
    "\n",
    "    for name, vectorizer in vectorizers.items():\n",
    "        print(f'Hyperparameter Tuning for {name}:\\n')\n",
    "\n",
    "        pipeline_steps = [\n",
    "            ('feature_engineering', vectorizer),\n",
    "            ('crossvalidate', GridSearchCV(trad_model, parameters,\n",
    "                                            cv=cv, refit=True, verbose=0, n_jobs=-1))\n",
    "        ]\n",
    "\n",
    "        if trad_model.__class__.__name__ == 'GaussianNB' and (name == 'CountVectorizer' or name == 'TfidfVectorizer'):\n",
    "            pipeline_steps.insert(1, ('to_dense', DenseTransformer()))\n",
    "\n",
    "        model = Pipeline(steps=pipeline_steps)\n",
    "        model.fit(X_train, y_train)\n",
    "        best_model = model.named_steps['crossvalidate'].best_params_\n",
    "\n",
    "        # Make prediction\n",
    "        ypred_train = model.predict(X_train)\n",
    "        ypred_test = model.predict(X_test)\n",
    "\n",
    "        best_preds_train[vectorizer] = ypred_train\n",
    "        best_preds_test[vectorizer] = ypred_test\n",
    "\n",
    "        print('=====================')\n",
    "        print(f'(1.) Vectorizer = {name}')\n",
    "        print(f'(2.) Model = {trad_model.__class__.__name__}')\n",
    "        print(f'(3.) Best Estimator = {best_model}')\n",
    "        print('=====================\\n')\n",
    "\n",
    "        res = {'Vectorizer': name,\n",
    "               'Model': trad_model.__class__.__name__,\n",
    "               'Train ACC': accuracy_score(y_train, ypred_train),\n",
    "               'Train B ACC': balanced_accuracy_score(y_train, ypred_train),\n",
    "               'Train F1': f1_score(y_train, ypred_train, average='weighted'),\n",
    "               'Test ACC': accuracy_score(y_test, ypred_test),\n",
    "               'Test B ACC': balanced_accuracy_score(y_test, ypred_test),\n",
    "               'Test F1': f1_score(y_test, ypred_test, average='weighted')}\n",
    "\n",
    "        results.append(res)\n",
    "\n",
    "        if res['Test ACC'] > current_best['accuracy']:\n",
    "            current_best['accuracy'] = res['Test ACC']\n",
    "            current_best['model'] = model\n",
    "            current_best['params'] = best_model\n",
    "\n",
    "        # Free up memory\n",
    "        del model\n",
    "        gc.collect()\n",
    "\n",
    "    print(f'Hyperparameter Tuning Complete!\\n')\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return {\n",
    "        'overall_best_model_result': current_best,\n",
    "        'metrics_df': results_df,\n",
    "        'best_preds_train': best_preds_train,\n",
    "        'best_preds_test': best_preds_test\n",
    "    }\n",
    "\n",
    "# Call the function with the pre-initialized vectorizers\n",
    "results = train_and_evaluate_models(\n",
    "    trad_model,\n",
    "    parameters,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    vectorizers=None,  # Use the default vectorizers\n",
    "    cv=pds\n",
    ")\n",
    "\n",
    "# Or, provide your own custom vectorizers\n",
    "custom_vectorizers = {\n",
    "    'CustomVectorizer': MyCustomVectorizer(),\n",
    "    # Add other custom vectorizers as needed\n",
    "}\n",
    "\n",
    "custom_results = train_and_evaluate_models(\n",
    "    trad_model,\n",
    "    parameters,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    vectorizers=custom_vectorizers,\n",
    "    cv=pds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Splits\n",
    "\n",
    "I will divide the training data into:\n",
    "\n",
    "1. Training Set: 90\\%\n",
    "2. Validation Set: 10\\%\n",
    "\n",
    "THe validation set will be used for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Validation data\n",
    "train_df, val_df = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "## Split X and y\n",
    "X_train, y_train = train_data['question'], train_data['category']\n",
    "X_test, y_test = test_data['question'], test_data['category']\n",
    "\n",
    "split_index = [-1 if x in train_df.index else 0 for x in train_data.index]\n",
    "pds = PredefinedSplit(test_fold = split_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42, solver='liblinear', max_iter=int(1e3))\n",
    "log_reg_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.1, 1, 10, 100, 1000],\n",
    "}\n",
    "\n",
    "lr_results = train_and_evaluate_models(\n",
    "    log_reg,\n",
    "    log_reg_params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    vectorizers=None,  # Use the default vectorizers\n",
    "    cv=pds\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
