{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Union\n",
    "\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, logging\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, PredefinedSplit\n",
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "    accuracy_score, \n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    ConfusionMatrixDisplay,\n",
    "    balanced_accuracy_score, \n",
    "    average_precision_score\n",
    ") \n",
    "\n",
    "logging.set_verbosity_error() # to avoid warning messages\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # set to false to avoid warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5452 entries, 0 to 5451\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   question      5452 non-null   object\n",
      " 1   category      5452 non-null   object\n",
      " 2   sub_category  5452 non-null   object\n",
      " 3   cleaned       5452 non-null   object\n",
      " 4   wordlengths   5452 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 213.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>wordlengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "      <td>serfdom develop leave russia</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>cremat</td>\n",
       "      <td>film feature character popeye doyle</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "      <td>list celebrity real name</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>animal</td>\n",
       "      <td>fowl grab spotlight chinese year monkey</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>ABBR</td>\n",
       "      <td>exp</td>\n",
       "      <td>form com</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question category sub_category  \\\n",
       "0  How did serfdom develop in and then leave Russ...     DESC       manner   \n",
       "1   What films featured the character Popeye Doyle ?     ENTY       cremat   \n",
       "2  How can I find a list of celebrities ' real na...     DESC       manner   \n",
       "3  What fowl grabs the spotlight after the Chines...     ENTY       animal   \n",
       "4                    What is the full form of .com ?     ABBR          exp   \n",
       "\n",
       "                                   cleaned  wordlengths  \n",
       "0             serfdom develop leave russia           10  \n",
       "1      film feature character popeye doyle            8  \n",
       "2                 list celebrity real name           12  \n",
       "3  fowl grab spotlight chinese year monkey           13  \n",
       "4                                 form com            8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_pickle('./data/train_data.pkl')\n",
    "test_data = pd.read_pickle('./data/test_data.pkl')\n",
    "\n",
    "train_data.info()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embedding Transformer\n",
    "\n",
    "We will now create a class for generating embeddings from text data using pre-trained language models. Specifically, we will use two language models:\n",
    "\n",
    "1. Bidirectional Encoder Representations from Transformers (BERT) Model by Google (read more [here](https://huggingface.co/google-bert/bert-base-uncased)).\n",
    "2. google/nnlm: Token based text embeddings trained on various Google News datasets (read more [here](https://www.kaggle.com/models/google/nnlm/frameworks/tensorFlow2/variations/en-dim50/versions/1?tfhub-redirect=true))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer for generating embeddings from text data using pre-trained language models.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained language model to use for generating embeddings.\n",
    "                          If \"nnlm-en-dim50\" is in the model_name, the corresponding TensorFlow Hub model will be used.\n",
    "                          Otherwise, a Hugging Face Transformers model will be used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"bert-base-uncased\"):\n",
    "        self.model_name = model_name\n",
    "        self._validate_model_name()\n",
    "\n",
    "        if \"nnlm-en-dim50\" in self.model_name:\n",
    "            self.embed = hub.load(\"https://www.kaggle.com/models/google/nnlm/frameworks/TensorFlow2/variations/en-dim128/versions/1\")\n",
    "        else:\n",
    "            self._setup_transformers_model()\n",
    "\n",
    "    def _validate_model_name(self):\n",
    "        \"\"\"Check if the specified model_name is valid.\"\"\"\n",
    "        if not isinstance(self.model_name, str):\n",
    "            raise ValueError(\"model_name must be a string\")\n",
    "\n",
    "    def _setup_transformers_model(self):\n",
    "        \"\"\"Load the Hugging Face Transformers model and set up the appropriate device.\"\"\"\n",
    "        self.device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name).to(self.device)\n",
    "\n",
    "    def fit(self, X: Union[pd.Series, pd.DataFrame], y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer (required for scikit-learn compatibility).\n",
    "\n",
    "        Args:\n",
    "            X (Union[pd.Series, pd.DataFrame]): The input data (not used for this transformer).\n",
    "            y (optional): The target data (not used for this transformer).\n",
    "\n",
    "        Returns:\n",
    "            self: The EmbeddingTransformer instance.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: Union[pd.Series, pd.DataFrame]):\n",
    "        \"\"\"\n",
    "        Generate embeddings from input text data using the specified pre-trained language model.\n",
    "\n",
    "        Args:\n",
    "            X (Union[pd.Series, pd.DataFrame]): The input text data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A Pandas DataFrame containing the generated embeddings.\n",
    "        \"\"\"\n",
    "        self._validate_input_data(X)\n",
    "\n",
    "        if \"nnlm-en-dim50\" in self.model_name:\n",
    "            return self._transform_with_nnlm(X)\n",
    "        else:\n",
    "            return self._transform_with_transformers(X)\n",
    "\n",
    "    def _validate_input_data(self, X: Union[pd.Series, pd.DataFrame]):\n",
    "        \"\"\"Check if the input data is valid.\"\"\"\n",
    "        if not isinstance(X, (pd.Series, pd.DataFrame)):\n",
    "            raise ValueError(\"Input data must be a Pandas Series or DataFrame\")\n",
    "\n",
    "    def _transform_with_nnlm(self, X: Union[pd.Series, pd.DataFrame]):\n",
    "        \"\"\"Generate embeddings using the TensorFlow Hub NNLM model.\"\"\"\n",
    "        text_data = X.tolist()\n",
    "        embeddings = self.embed(text_data).numpy()\n",
    "        return pd.DataFrame(embeddings)\n",
    "\n",
    "    def _transform_with_transformers(self, X: Union[pd.Series, pd.DataFrame]):\n",
    "        \"\"\"Generate embeddings using a Hugging Face Transformers model.\"\"\"\n",
    "        batch_text = X.tolist()\n",
    "        inputs = self.tokenizer.batch_encode_plus(\n",
    "            batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=22\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        return pd.DataFrame(embeddings)\n",
    "\n",
    "## create DenseTransformer class to convert sparse matrix to dense matrix\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 13:02:18.717322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1073 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-03-10 13:02:18.717940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21280 MB memory:  -> device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Initialize the vectorizers\n",
    "default_vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(stop_words='english', ngram_range=(1, 2), max_features=1024),\n",
    "    'TfidfVectorizer': TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=1024),\n",
    "    'bert-base-uncased': EmbeddingTransformer('bert-base-uncased'),\n",
    "    'nnlm-en-dim128': EmbeddingTransformer('nnlm-en-dim50')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Text Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(\n",
    "    trad_model,\n",
    "    parameters,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    vectorizers=None,\n",
    "    cv: Optional[PredefinedSplit] = None,\n",
    "):\n",
    "    results = []\n",
    "    best_preds_train = {}\n",
    "    best_preds_test = {}\n",
    "    current_best = {\n",
    "        'accuracy': 0,\n",
    "        'model': None,\n",
    "        'params': None\n",
    "    }\n",
    "\n",
    "    if vectorizers is None:\n",
    "        vectorizers = default_vectorizers\n",
    "\n",
    "    for name, vectorizer in vectorizers.items():\n",
    "        print(f'Hyperparameter Tuning for {name}:\\n')\n",
    "\n",
    "        pipeline_steps = [\n",
    "            ('feature_engineering', vectorizer),\n",
    "            ('crossvalidate', GridSearchCV(trad_model, parameters,\n",
    "                                            cv=cv, refit=True, verbose=0, n_jobs=-1))\n",
    "        ]\n",
    "\n",
    "        if trad_model.__class__.__name__ == 'GaussianNB' and (name == 'CountVectorizer' or name == 'TfidfVectorizer'):\n",
    "            pipeline_steps.insert(1, ('to_dense', DenseTransformer()))\n",
    "\n",
    "        model = Pipeline(steps=pipeline_steps)\n",
    "        model.fit(X_train, y_train)\n",
    "        best_model = model.named_steps['crossvalidate'].best_params_\n",
    "\n",
    "        # Make prediction\n",
    "        ypred_train = model.predict(X_train)\n",
    "        ypred_test = model.predict(X_test)\n",
    "\n",
    "        best_preds_train[vectorizer] = ypred_train\n",
    "        best_preds_test[vectorizer] = ypred_test\n",
    "\n",
    "        print('=====================')\n",
    "        print(f'(1.) Vectorizer = {name}')\n",
    "        print(f'(2.) Model = {trad_model.__class__.__name__}')\n",
    "        print(f'(3.) Best Estimator = {best_model}')\n",
    "        print('=====================\\n')\n",
    "\n",
    "        res = {'Vectorizer': name,\n",
    "               'Model': trad_model.__class__.__name__,\n",
    "               'Train ACC': accuracy_score(y_train, ypred_train),\n",
    "               'Train B ACC': balanced_accuracy_score(y_train, ypred_train),\n",
    "               'Train F1': f1_score(y_train, ypred_train, average='weighted'),\n",
    "               'Test ACC': accuracy_score(y_test, ypred_test),\n",
    "               'Test B ACC': balanced_accuracy_score(y_test, ypred_test),\n",
    "               'Test F1': f1_score(y_test, ypred_test, average='weighted')}\n",
    "\n",
    "        results.append(res)\n",
    "\n",
    "        if res['Test B ACC'] > current_best['accuracy']: ## model selection based on balanced accuracy\n",
    "            current_best['balanced_accuracy'] = res['Test B ACC']\n",
    "            current_best['model'] = model\n",
    "            current_best['params'] = best_model\n",
    "\n",
    "        # Free up memory\n",
    "        del model\n",
    "        gc.collect()\n",
    "\n",
    "    print(f'Hyperparameter Tuning Complete!\\n')\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return {\n",
    "        'overall_best_model_result': current_best,\n",
    "        'metrics_df': results_df,\n",
    "        'best_preds_train': best_preds_train,\n",
    "        'best_preds_test': best_preds_test\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Pipeline\n",
    "\n",
    "Text Classification Pipeline\n",
    "\n",
    "The `train_and_evaluate_models` demonstrates a machine learning pipeline for text classification using various\n",
    "vectorization techniques and traditional models. The following steps will be followed to use ths code.\n",
    "\n",
    "1. Data Preparation (this will only be done once):\n",
    "   - Split the original train data into training and validation sets.\n",
    "   - Separate the text data (X) and target labels (y) from the input data.\n",
    "\n",
    "2. Vectorization (defaults to `default_vectorizers`):\n",
    "   - Define several vectorization techniques, including:\n",
    "     - CountVectorizer: Converts text data into a matrix of token counts.\n",
    "     - TfidfVectorizer: Converts text data into a matrix of TF-IDF features.\n",
    "     - EmbeddingTransformer: Generates embeddings from text data using pre-trained language models\n",
    "       (e.g., BERT, NNLM).\n",
    "\n",
    "3. Model Training and Evaluation:\n",
    "   - Call the `train_and_evaluate_models` function that takes a traditional machine learning model,\n",
    "     its hyperparameters, and the training/test data as input.\n",
    "   - The function creates a pipeline with the specified vectorizer, model, and hyperparameters.\n",
    "   - Grid search is performed to find the best hyperparameters for the model using cross-validation.\n",
    "   - The pipeline is trained on the training data and evaluated on both the training and test data.\n",
    "   - Performance metrics (accuracy, balanced accuracy, and F1-score) are calculated and stored.\n",
    "   - The best model and its hyperparameters are tracked and returned.\n",
    "\n",
    "4. Usage:\n",
    "   - We will demonstrate the usage of the `train_and_evaluate_models` function with \n",
    "      * Logistic Regression model.\n",
    "      * Gaussian Naive Bayes, and\n",
    "      * Extremely Randomized Trees (ExtraTrees).\n",
    "   - The results, including the best model, performance metrics, and predictions, are stored in a dictionary.\n",
    "\n",
    "This pipeline can be extended or modified to include additional vectorization techniques, machine learning\n",
    "models, or preprocessing steps as needed for your specific text classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Splits\n",
    "\n",
    "I will divide the training data into:\n",
    "\n",
    "1. Training Set: 90\\%\n",
    "2. Validation Set: 10\\%\n",
    "\n",
    "THe validation set will be used for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Validation data\n",
    "train_df, val_df = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "## Split X and y\n",
    "X_train, y_train = train_data['question'], train_data['category']\n",
    "X_test, y_test = test_data['question'], test_data['category']\n",
    "\n",
    "split_index = [-1 if x in train_df.index else 0 for x in train_data.index]\n",
    "pds = PredefinedSplit(test_fold = split_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning for CountVectorizer:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "(1.) Vectorizer = CountVectorizer\n",
      "(2.) Model = LogisticRegression\n",
      "(3.) Best Estimator = {'C': 1, 'penalty': 'l2'}\n",
      "=====================\n",
      "\n",
      "Hyperparameter Tuning for TfidfVectorizer:\n",
      "\n",
      "=====================\n",
      "(1.) Vectorizer = TfidfVectorizer\n",
      "(2.) Model = LogisticRegression\n",
      "(3.) Best Estimator = {'C': 1, 'penalty': 'l2'}\n",
      "=====================\n",
      "\n",
      "Hyperparameter Tuning for bert-base-uncased:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=42, solver='liblinear', max_iter=int(1e3))\n",
    "log_reg_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.1, 1, 10, 100, 1000],\n",
    "}\n",
    "\n",
    "lr_results = train_and_evaluate_models(\n",
    "    log_reg,\n",
    "    log_reg_params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    vectorizers=None,  # Use the default vectorizers\n",
    "    cv=pds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Model</th>\n",
       "      <th>Train ACC</th>\n",
       "      <th>Train B ACC</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test ACC</th>\n",
       "      <th>Test B ACC</th>\n",
       "      <th>Test F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.759721</td>\n",
       "      <td>0.744048</td>\n",
       "      <td>0.762112</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.692910</td>\n",
       "      <td>0.686102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.743213</td>\n",
       "      <td>0.722536</td>\n",
       "      <td>0.744480</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.705807</td>\n",
       "      <td>0.694875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.942040</td>\n",
       "      <td>0.921940</td>\n",
       "      <td>0.941905</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.918115</td>\n",
       "      <td>0.924167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nnlm-en-dim128</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.812724</td>\n",
       "      <td>0.775811</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.818056</td>\n",
       "      <td>0.805953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Vectorizer               Model  Train ACC  Train B ACC  Train F1  \\\n",
       "0    CountVectorizer  LogisticRegression   0.759721     0.744048  0.762112   \n",
       "1    TfidfVectorizer  LogisticRegression   0.743213     0.722536  0.744480   \n",
       "2  bert-base-uncased  LogisticRegression   0.942040     0.921940  0.941905   \n",
       "3     nnlm-en-dim128  LogisticRegression   0.775862     0.812724  0.775811   \n",
       "\n",
       "   Test ACC  Test B ACC   Test F1  \n",
       "0     0.694    0.692910  0.686102  \n",
       "1     0.704    0.705807  0.694875  \n",
       "2     0.926    0.918115  0.924167  \n",
       "3     0.808    0.818056  0.805953  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_results['metrics_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Configuration (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.926,\n",
       " 'model': Pipeline(steps=[('feature_engineering', EmbeddingTransformer()),\n",
       "                 ('crossvalidate',\n",
       "                  GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
       "                               estimator=LogisticRegression(max_iter=1000,\n",
       "                                                            random_state=42,\n",
       "                                                            solver='liblinear'),\n",
       "                               n_jobs=-1,\n",
       "                               param_grid={'C': [0.001, 0.1, 1, 10, 100, 1000],\n",
       "                                           'penalty': ['l1', 'l2']}))]),\n",
       " 'params': {'C': 1, 'penalty': 'l2'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Best Test Accuracy: {lr_results['overall_best_model_result']['accuracy']}\\n\")\n",
    "\n",
    "lr_results['overall_best_model_result'][]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
